---
title: Support Vector Machines (SVMs)
author: 
    name: Florin Bidian
    affiliations: Northeastern University
date: 11-30-2023
categories: [Machine Learning]
citation:
    url: https://bidianf.github.io/posts/2023-11-30-support-vector-machines
draft: false # `true` will prevent post from being listed until ready
image: img/hanauma.jpg
format: 
    html:
        code-fold: true
---

SVMs are among the most researched and popular classifiers, and have deep connections with fundamental results in functional analysis. Unlike  logistic regression, they work also when the classes are linearly separable, and through _kernelization_, they can identify nonlinear boundaries between classes.

Consider observations $(y_i,x_i)_{i=1}^n$
with $y \in \{-1,1\}$ being the dependent variable and $x \in \mathbb{R}^p$
the covariates.  A hyperplane in $\mathbb{R}^p$ is the set of points $x \in \mathbb{R}^p$ with the
property that $h(x)=0$, where $h : \mathbb{R}^p \to \mathbb{R}$ is an affine function $h(x):=\beta x +\beta_0$ with 
$\beta \in \mathbb{R}^p, \beta_0 \in \mathbb{R}$ , and
$$\beta x=\langle \beta,x \rangle := \sum_{j=1}^p \beta_j x_j$$  {#eq-svm-dot}
is the inner (dot) product of the
vectors $\beta=(\beta_1,\ldots,\beta_p)$ and $x=(x_1,\ldots,x_p)$. 

A hyperplane divides the $\mathbb{R}^p$ space into two half-spaces.
SVMs find the hyperplane that "best separates" 
the two classes $\{-1,1\}$. Roughly,  (most of the) class $1$ observations fall in the half-space $\{x\: |\: h(x)>=0\}$, while (most of the) class $-1$ observations belong to the half-space $\{x \: | \: h(x)<0\}$.  A new observation with covariates $x$ is will be classified as belonging to
class determined by the sign of $h(x) = \beta x +\beta_0$. While SVMs apply directly only to binary classes, they can be turned into a multiclass classifier using the one-vs-one pairwise classifications of observations, and using majority voting to determine the most likely class.

## Linearly separated classes

The two classes can be perfectly separated by the hyperplane $h$ if $y_i h(x_i) \ge 0$ for all $1 \le i \le n$.  
Denote by $\lVert \beta \rVert := (\sum_{j=1}^p \beta_j^2)^{1/2}$ the Euclidean norm of $\beta$. The distance from a
point $x$ to the hyperplane $h$ can be calculated by selecting a point $x_0$
belonging to the hyperplane and projecting $x-x_0$ on the unit norm
vector $\beta/\lVert \beta \rVert$ orthogonal to the hyperplane, 
$$
\frac{\lvert  \beta (x-x_0)\rvert}{\lVert \beta \rVert}   = \frac{\lvert \beta x +\beta_0 \rvert}{\lVert \beta \rVert} = \frac{\lvert h(x) \rvert}{\lVert \beta \rVert}.
$$ {#eq-svm-dist}

If $\beta$ is normalized to have unit norm,  $y_i h(x_i)$ is the distance from the observation $(y_i,x_i)$ to the hyperplane, or the _margin_ for this observation. 
With perfectly separable classes, we want to find the *optimal-margin* hyperplane, which is the hyperplane with the largest distance even from the closest
points:
$$
\begin{aligned}
\max_{\beta,\beta_0,M} M  \\
\text{s.t. } y_i  \frac{\beta x_i +\beta_0}{\lVert \beta \rVert} \ge M. 
\end{aligned} 
$$ {#eq-svm-margin}
@fig-svm-perfect-separation illustrates the case of perfectly separable observations lying the plane ($p=2$), where hyperplanes are lines. The observations with optimal margin which is the distance to the hyperplane marked by the blue line, are circled. They are called _support vectors_, giving the algorithm its name. The support vectors determine entirely the optimal-margin hyperplane and any points with higher margin (further from the separating hyperplane) do not affect the optimal solution to @{eq-svm-margin}.
```{python}
#| label: fig-svm-perfect-separation
#| fig-cap: "Perfect separation "

import numpy as np
import matplotlib.pyplot as plt
# Rotation matrix, 45 degrees counterclockwise
a = 2**.5/2
A = np.array([[a,-a],[a,a]])
rng = np.random.default_rng(10000)

def gen_data(spread=.2,nobs=100,rng=rng):
        X = (2*rng.random((nobs,2))-1) 
        #Obs x=(x_1,x_2) assigned to class 1 (-1) if x_2 >= (<) 0
        y =2*(X[:,1]>=0)-1
        #Inject spread between classes
        X[X[:,1]>=0,1]+=spread/2; X[X[:,1]<0,1]-=spread/2
        if spread>0:
            # Support vectors 
            svidx1 = rng.choice(np.where(X[:,1]>0)[0],2,replace=False)
            X[svidx1,1] = spread/2
            svidx_1 = rng.choice(np.where(X[:,1]<0)[0],1,replace=False)
            X[svidx_1,1] = -spread/2
            svidx=np.concatenate((svidx1,svidx_1))
            return (np.dot(X,A.T),y,spread/2,svidx)
        else:
            return (np.dot(X,A.T),y,spread/2,None)

X,y,M,svidx = gen_data()
fig,ax = plt.subplots()
ax.scatter(X[:, 0], X[:, 1], c=y, s=50)
xfit = yfit = np.linspace(-2**.5, 2**.5)
ax.plot(xfit, yfit,color='blue')
ax.set_xlim(-2**.5, 2**.54)
ax.fill_between(xfit, yfit - M/a, yfit + M/a, edgecolor='none',
                  color='lightgray', alpha=0.5)
ax.scatter(X[svidx, 0],
                    X[svidx, 1],
                    s=150, linewidth=1, edgecolors='black',
                    facecolors='none')
plt.show()
```
 

Relabel $\beta'_0=\beta_0/(M \lVert \beta \rVert)$, 
$\beta'=\beta/(M \lVert \beta \rVert)$, implying that $\lVert \beta' \rVert = 1/M$. Dropping the apostrophe, @eq-svm-margin rewrites as 
$$\begin{aligned}
\min_{\beta,\beta_0} \; \lVert \beta \rVert \\
\text{s.t. } y_i  (\beta x_i +\beta_0)  \ge 1. \end{aligned}$$ {#eq-svm-margin-substituted}

## Overlapping classes

In general, perfect separation is impossible and we are content to find  a "soft"-margin classifier solving  the relaxed problem
$$\begin{aligned}
\min_{\beta,\beta_0,\xi_i} \frac 12 \lVert \beta \rVert^2 + C \sum_{i=1}^n \xi_i,  \label{eq:rkhs:11}\\
\text{s.t. } \xi_i \ge 0, \; y_i(x_i \beta +\beta_0) \ge 1-\xi_i, \forall i \notag \end{aligned}$$ {#eq-svm-soft-margin}
The tuning parameter $C$ controls the softness of the margin, and %(\xi_i)$ are called _slack variables_. A large $C$ implies less overlap.

@eq-svm-soft-margin can be rewritten as an unconstrained optimization problem,
$$
\min_{\beta,\beta_0}  \frac 12 \lVert \beta \rVert^2 +C \sum_i [1- y_i(x_i \beta +\beta_0) ]_+,$$ {#eq-svm-soft-unconstrained}
or, in the familiar form of minimizing a penalized loss function,
$$
\min_{\beta,\beta_0}  \sum_i [1- y_i(x_i \beta +\beta_0) ]_+ + \lambda \lVert \beta \rVert^2,$$ {#eq-svm-soft-hingeloss}
where $[1-y_i h(x_i)]_+:=\max\{1-y_i h(x_i),0\}$ is known as the *hinge
loss*, operating on the margin $y_i h(x_i)=y_i(x_i \beta +\beta_0)$. It
measures the cost of $x_i$ being on the wrong side of the separating
hyperplane.

The class below implements from scratch an SVC in `python`, using only `numpy`. It mimics the widely used interface of `sklearn`, and solves the problem in  @eq-svm-soft-hingeloss using either _stochastic gradient descent (SGD)_ or the _BFGS_ optimizer in `scipy`.
```{python} 
from dataclasses import dataclass
from scipy.optimize import minimize

@dataclass
class svm:
    lambd : float=1/1e10
    grad_step : float=0.001
    nepochs : int=1000
    
    def loss(self,coeff,X,y):
        n,p= X.shape
        alpha=coeff[0];beta=coeff[1:]
        loss=0
        for i,xi in enumerate(X):
            hingeloss=1-y[i]*(alpha+np.dot(xi,beta))
            loss+=hingeloss*(hingeloss>=0)  
        return loss/n+2*self.lambd*np.sum(beta**2)
        
    def fit_bfgs(self,X,y):
        coeff=np.zeros(X.shape[1]+1)
        res = minimize(self.loss,coeff,method='BFGS',args=(X,y),
                       tol = 1e-6, 
                       options={'gtol': 1e-6, 'disp': False,'maxiter':300})
        self.beta = res.x[1:]
        self.alpha = res.x[0] 
        self.loss = [res.fun]
        return self
    
    def fit_sgd(self,X,y):
        n,p=X.shape
        self.alpha=1
        self.beta=np.ones(p)
        y=y.reshape(-1,1)
        for _ in range(self.nepochs):
            for i,xi in enumerate(X):
                hingeloss = 1-y[i]*(self.alpha+np.dot(xi,self.beta))
                dalpha=-y[i]*(hingeloss>=0)
                dbeta= -y[i]*xi*(hingeloss>=0)+2*self.lambd*self.beta
                self.alpha-=self.grad_step * dalpha
                self.beta-=self.grad_step * dbeta
        return self
    
    def predict(self,X):
        return np.where(self.alpha+X.dot(self.beta)>=0,1,-1)
        
```
For the separated observations case in @fig-svm-perfect-separation, both the SGD and BFGS produce separating hyperplanes with intercepts close to zero and slopes close to 1, as expected, similar to  `scikit-learn`, which uses the dual problem @eq-svm-dual to be detailed next, instead of @eq-svm-soft-hingeloss
```{python} 
#| label: tbl-svc
#| tbl-cap: Optimal-margin hyperplane using SGD, BFGS and the scikit-learn implementation of SVC. Exact values are 0 and 1.

import warnings
from IPython.display import Markdown
from tabulate import tabulate
from sklearn.svm import SVC # "Support vector classifier"


warnings.simplefilter('ignore')

def run_models():
    m1=svm()
    m1.fit_sgd(X,y)
    intercept1 = -m1.alpha/m1.beta[1]
    slope1 = - m1.beta[0]/m1.beta[1]
    m2=svm()
    m2.fit_bfgs(X,y)
    intercept2 = -m2.alpha/m2.beta[1]
    slope2 = - m2.beta[0]/m2.beta[1]

    m3 = SVC(kernel='linear', C=1E10)
    m3.fit(X, y)
    intercept3 = m3.intercept_
    slope3 = - m3.coef_[0,0]/m3.coef_[0,1]

    return (intercept1,slope1,intercept2,slope2,intercept3,slope3)

intercept1,slope1,intercept2,slope2,intercept3,slope3 =run_models()

table = [['SGD',intercept1,slope1],['BFGS',intercept2,slope2],['sklearn',intercept3,slope3],['exact',0,1]]
Markdown(tabulate(
  table, 
  headers=["Method","Intercept", "Slope"]
))
```
@fig-svm-overlap illustrates the SGD and BFGS-based SVM routine applied to overlapping observations:

```{python}
#| label: fig-svm-overlap
#| fig-cap: "Overlapping classes "

X,y,M,svidx = gen_data(spread = -.2)
fig,ax = plt.subplots()
ax.scatter(X[:, 0], X[:, 1], c=y, s=50)
L = run_models()
labels = ['SGD','BFGS']
colors = ['blue','green']
xfit = np.linspace(-2**.5, 2**.5)
for i in range(2):
    ax.plot(xfit,L[2*i]+L[2*i+1]*xfit,color = colors[i],label=labels[i])
ax.legend()

plt.show()
```


## Nonlinear boundaries

SVMs can be extended to identify highly non-linear boundaries through _kernelization_ (the "kernel trick"). The idea is to map the covariate space $\mathbb{R}^p$ into a higher dimensional space $H$, and find a separating hyperplane in the bigger space. For example, with 1-dimensional covariates, if observations in class $1$ fall in the interval $(-1,1)$ while observations of class $-1$ belong to $(-3,-2) \cup (2,3)$, they cannot be linearly separated (cutting the real line in two). However, if we map the covariate $x$ into a point $(x,x^2)$ in a 2-dimensional space, any horizontal line with height between 1 and 4 linearly separates the observations. 


We show in the next section that given a  function $K: \mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}$ with some suitable properties (a _kernel_), there exists a  (possibly infinitely dimensional) Hilbert space H - the _reproducing kernel Hilbert space (RKHS)_ and  a mapping $\Psi$ from the original feature space $\mathbb{R}^p$ to $H$ with the property that the inner products in the new space have the _reproducing property_ in that they can be directly calculated using $K$,
$$
\langle \Psi(x),\Psi(y) \rangle = K(x,y).
$$ {#eq-svm-reproducing-property}

We solve now @eq-svm-soft-margin in this higher dimensional space, with $x_i$ replaced by $\Psi(x_i)$. Heuristically, I solve
the problem as if it remains finite dimensional. Let $(\alpha_i)$ be the
Lagrange multipliers for constraints, and $\mu_i$ the multiplier for
$\xi_i$. The Lagrangian is 
$$
L:=\frac 12 \lVert \beta \rVert^2 + C \sum_{i=1}^n \xi_i - \sum_i \alpha_i (y_i(\Psi(x_i) \beta +\beta_0) - 1+\xi_i)  -\sum_i \mu_i \xi_i.$$ {#eq-svm-lagrange}

The first order conditions with respect to $\beta, \beta_0, \xi_i$ give
$$
\beta = \sum_{i=1}^n \alpha_i y_i \Psi(x_i), \sum_i \alpha_i y_i =0, C =\alpha_i +\mu_i.$$ {#eq-svm-foc}
Substituting @eq-svm-foc into @eq-svm-lagrange,
$$\begin{aligned}
L =  \frac 12 \sum_i \sum_j \alpha_i \alpha_j y_i   y_j \langle \Psi(x_i),\Psi(x_j) \rangle - \sum_i \alpha_i (y_i \Psi(x_i)\sum_j \alpha_j y_j \Psi(x_j) -1) \\
=\sum_i \alpha_i - \frac 12 \sum_i \sum_j \alpha_i \alpha_j y_i   y_j  K(x_i,x_j).\end{aligned}$$ {#eq-svm-lagrange2}

@eq-svm-lagrange2 depends only on the $n \times n$  _Gram matrix_ of inner products  $(\langle \Psi(x_i),\Psi(x_j) \rangle)$, or equivalently, on $(K(x_i,x_j))$. We obtained the equivalent, dual form of @eq-svm-soft-margin generalized to non-linear boundaries,
$$\begin{aligned}
\min_{\alpha_i} \sum_i \alpha_i - \frac 12 \sum_i \sum_j \alpha_i \alpha_j y_i   y_j  K(x_i,x_j) \\
\text{s.t. } \sum_i \alpha_i y_i =0,  0 \le \alpha_i \le C \quad \forall i .\end{aligned}$$ {#eq-svm-dual}

Dedicated optimization routines easily handle such
quadratic problems with linear and box constraints.  @fig-svm-nonlinear  illustrates how a SVM with radial kernel
$$
K(x,y) = \exp(- \lVert x-y \rVert^2 /(2\sigma^2))
$$ {#eq-svm-radial-kernel}
is able to detect nonlinear boundaries between classes.

```{python}
#| label: fig-svm-nonlinear
#| fig-cap: "Nonlinear boundary"

from sklearn.datasets import make_circles

X, y = make_circles(100, factor=.1, noise=.1)

model = SVC(kernel='rbf').fit(X, y)

fig,ax = plt.subplots()
plt.scatter(X[:, 0], X[:, 1], c=y, s=50)


xlim = ax.get_xlim()
ylim = ax.get_ylim()


# make grid
x = np.linspace(xlim[0], xlim[1], 20)
y = np.linspace(ylim[0], ylim[1], 20)
Y, X = np.meshgrid(y, x)
xy = np.concatenate((X.reshape(-1,1), Y.reshape(-1,1)), axis = 1)
H = model.decision_function(xy).reshape(X.shape)

# plot decision boundary and margins
ax.contour(X, Y, H, levels=[0], alpha=0.5)
plt.show()
```



## Reproducing Kernel Hilbert Space (RKHS) and the Kernel Trick

Let $X \subset \mathbb{R}^p$ be the set of possible covariates. 
A two-dimensional function $K(x,y):X \times X \to \mathbb{R}$ can be
interpreted as an infinite matrix, in the same way that a function
from $X$ to $\mathbb{R}$ can be viewed as a vector.
Therefore, the matrix terminology carries over. We say that
$K(x,y)$ is a *kernel* function if it is symmetric and positive
definite, that is, if $K(x,y)=K(y,x)$ and
$\int \int f(x) K(x,y) f(y)dx dy \ge 0$ for all $f$.

Proving that $K$ in @eq-svm-dual is positive definite is usually done by proving the various properties of kernels. Scaling, sums, products, limits, powers and exponentiation of kernels preserve the kernel property, and the radial kernel can be obtained from the linear kernel $\tilde{K}(x,y) = \langle x,y \rangle$.

Analogous to matrices, $K$ has  (a countable number of) eigenvalues $(\lambda_n)$ and eigenvectors $(\psi_n)$ with $\psi_n:X \rightarrow \mathbb{R}$ satisfying
$$\int K(x,y) \psi_n(y) dy =\lambda_n \psi_n(x), \text{for all } x,n.$$ {#eq-svm-eigen}
By the usual arguments, the eigenvalues are real (by symmetry) and positive (by the positive definite property), and the 
eigenvectors are orthogonal with respect to the inner product
$$
\langle \psi_i, \psi_j \rangle := \int \psi_i(x) \psi_j(x) dx =0, \; i \neq j.
$$ {#eq-svm-eigenvectors}

Mercer's theorem states that, as in the matrix case,  a kernel admits a spectral decomposition,

$$
K(x,y) = \sum_{n=1}^\infty \lambda_n \psi_n(x) \psi_n(y),$$ {#eq-svm-spectral}
This ensures that $(\phi_n)$ is an orthogonal basis for the space of functions
$H:=\{K(x,\cdot) \; : \; x \in X \}$, since
$$
K(x,\cdot) =  \sum_{n=1}^\infty \lambda_n \psi_n(x) \psi_n.$$ {#eq-svm-basis-decomposition}
As long as each eigenvector  $\psi_n$ is rescaled to have norm $1/\sqrt{\lambda_n}$, the inner product on $H$ has the *reproducing
property*
$$
\langle K(x,\cdot),K(y,\cdot) \rangle= \sum_{n=1}^\infty \lambda^2_n \psi_n(x) \psi_n(y) \langle \psi_n,\psi_n \rangle = \sum_{n=1}^\infty \lambda_n \psi_n(x) \psi_n(y) =  K(x,y).$$ {#eq-svm-inner-product-reproducing}

