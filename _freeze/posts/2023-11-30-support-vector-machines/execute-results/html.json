{
  "hash": "788a11746271e36b473ba6d7572c2293",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Support Vector Machines (SVMs)\nauthor: \n    name: Florin Bidian\n    affiliations: Northeastern University\ndate: 11-30-2023\ncategories: [Machine Learning]\ncitation:\n    url: https://bidianf.github.io/posts/2023-11-30-support-vector-machines\ndraft: false # `true` will prevent post from being listed until ready\nimage: img/hanauma.jpg\nformat: \n    html:\n        code-fold: true\n---\n\nSVMs are among the most researched and popular classifiers, and have deep connections with fundamental results in functional analysis. Unlike  logistic regression, they work also when the classes are linearly separable, and through _kernelization_, they can identify nonlinear boundaries between classes.\n\nConsider observations $(y_i,x_i)_{i=1}^n$\nwith $y \\in \\{-1,1\\}$ being the dependent variable and $x \\in \\mathbb{R}^p$\nthe covariates.  A hyperplane in $\\mathbb{R}^p$ is the set of points $x \\in \\mathbb{R}^p$ with the\nproperty that $h(x)=0$, where $h : \\mathbb{R}^p \\to \\mathbb{R}$ is an affine function $h(x):=\\beta x +\\beta_0$ with \n$\\beta \\in \\mathbb{R}^p, \\beta_0 \\in \\mathbb{R}$ , and\n$$\\beta x=\\langle \\beta,x \\rangle := \\sum_{j=1}^p \\beta_j x_j$$  {#eq-svm-dot}\nis the inner (dot) product of the\nvectors $\\beta=(\\beta_1,\\ldots,\\beta_p)$ and $x=(x_1,\\ldots,x_p)$. \n\nA hyperplane divides the $\\mathbb{R}^p$ space into two half-spaces.\nSVMs find the hyperplane that \"best separates\" \nthe two classes $\\{-1,1\\}$. Roughly,  (most of the) class $1$ observations fall in the half-space $\\{x\\: |\\: h(x)>=0\\}$, while (most of the) class $-1$ observations belong to the half-space $\\{x \\: | \\: h(x)<0\\}$.  A new observation with covariates $x$ is will be classified as belonging to\nclass determined by the sign of $h(x) = \\beta x +\\beta_0$. While SVMs apply directly only to binary classes, they can be turned into a multiclass classifier using the one-vs-one pairwise classifications of observations, and using majority voting to determine the most likely class.\n\n## Linearly separated classes\n\nThe two classes can be perfectly separated by the hyperplane $h$ if $y_i h(x_i) \\ge 0$ for all $1 \\le i \\le n$.  \nDenote by $\\lVert \\beta \\rVert := (\\sum_{j=1}^p \\beta_j^2)^{1/2}$ the Euclidean norm of $\\beta$. The distance from a\npoint $x$ to the hyperplane $h$ can be calculated by selecting a point $x_0$\nbelonging to the hyperplane and projecting $x-x_0$ on the unit norm\nvector $\\beta/\\lVert \\beta \\rVert$ orthogonal to the hyperplane, \n$$\n\\frac{\\lvert  \\beta (x-x_0)\\rvert}{\\lVert \\beta \\rVert}   = \\frac{\\lvert \\beta x +\\beta_0 \\rvert}{\\lVert \\beta \\rVert} = \\frac{\\lvert h(x) \\rvert}{\\lVert \\beta \\rVert}.\n$$ {#eq-svm-dist}\n\nIf $\\beta$ is normalized to have unit norm,  $y_i h(x_i)$ is the distance from the observation $(y_i,x_i)$ to the hyperplane, or the _margin_ for this observation. \nWith perfectly separable classes, we want to find the *optimal-margin* hyperplane, which is the hyperplane with the largest distance even from the closest\npoints:\n$$\n\\begin{aligned}\n\\max_{\\beta,\\beta_0,M} M  \\\\\n\\text{s.t. } y_i  \\frac{\\beta x_i +\\beta_0}{\\lVert \\beta \\rVert} \\ge M. \n\\end{aligned} \n$$ {#eq-svm-margin}\n@fig-svm-perfect-separation illustrates the case of perfectly separable observations lying the plane ($p=2$), where hyperplanes are lines. The observations with optimal margin which is the distance to the hyperplane marked by the blue line, are circled. They are called _support vectors_, giving the algorithm its name. The support vectors determine entirely the optimal-margin hyperplane and any points with higher margin (further from the separating hyperplane) do not affect the optimal solution to @{eq-svm-margin}.\n\n::: {#cell-fig-svm-perfect-separation .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Rotation matrix, 45 degrees counterclockwise\na = 2**.5/2\nA = np.array([[a,-a],[a,a]])\nrng = np.random.default_rng(10000)\n\ndef gen_data(spread=.2,nobs=100,rng=rng):\n        X = (2*rng.random((nobs,2))-1) \n        #Obs x=(x_1,x_2) assigned to class 1 (-1) if x_2 >= (<) 0\n        y =2*(X[:,1]>=0)-1\n        #Inject spread between classes\n        X[X[:,1]>=0,1]+=spread/2; X[X[:,1]<0,1]-=spread/2\n        if spread>0:\n            # Support vectors \n            svidx1 = rng.choice(np.where(X[:,1]>0)[0],2,replace=False)\n            X[svidx1,1] = spread/2\n            svidx_1 = rng.choice(np.where(X[:,1]<0)[0],1,replace=False)\n            X[svidx_1,1] = -spread/2\n            svidx=np.concatenate((svidx1,svidx_1))\n            return (np.dot(X,A.T),y,spread/2,svidx)\n        else:\n            return (np.dot(X,A.T),y,spread/2,None)\n\nX,y,M,svidx = gen_data()\nfig,ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nxfit = yfit = np.linspace(-2**.5, 2**.5)\nax.plot(xfit, yfit,color='blue')\nax.set_xlim(-2**.5, 2**.54)\nax.fill_between(xfit, yfit - M/a, yfit + M/a, edgecolor='none',\n                  color='lightgray', alpha=0.5)\nax.scatter(X[svidx, 0],\n                    X[svidx, 1],\n                    s=150, linewidth=1, edgecolors='black',\n                    facecolors='none')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Perfect separation](2023-11-30-support-vector-machines_files/figure-html/fig-svm-perfect-separation-output-1.png){#fig-svm-perfect-separation width=582 height=411}\n:::\n:::\n\n\nRelabel $\\beta'_0=\\beta_0/(M \\lVert \\beta \\rVert)$, \n$\\beta'=\\beta/(M \\lVert \\beta \\rVert)$, implying that $\\lVert \\beta' \\rVert = 1/M$. Dropping the apostrophe, @eq-svm-margin rewrites as \n$$\\begin{aligned}\n\\min_{\\beta,\\beta_0} \\; \\lVert \\beta \\rVert \\\\\n\\text{s.t. } y_i  (\\beta x_i +\\beta_0)  \\ge 1. \\end{aligned}$$ {#eq-svm-margin-substituted}\n\n## Overlapping classes\n\nIn general, perfect separation is impossible and we are content to find  a \"soft\"-margin classifier solving  the relaxed problem\n$$\\begin{aligned}\n\\min_{\\beta,\\beta_0,\\xi_i} \\frac 12 \\lVert \\beta \\rVert^2 + C \\sum_{i=1}^n \\xi_i,  \\label{eq:rkhs:11}\\\\\n\\text{s.t. } \\xi_i \\ge 0, \\; y_i(x_i \\beta +\\beta_0) \\ge 1-\\xi_i, \\forall i \\notag \\end{aligned}$$ {#eq-svm-soft-margin}\nThe tuning parameter $C$ controls the softness of the margin, and %(\\xi_i)$ are called _slack variables_. A large $C$ implies less overlap.\n\n@eq-svm-soft-margin can be rewritten as an unconstrained optimization problem,\n$$\n\\min_{\\beta,\\beta_0}  \\frac 12 \\lVert \\beta \\rVert^2 +C \\sum_i [1- y_i(x_i \\beta +\\beta_0) ]_+,$$ {#eq-svm-soft-unconstrained}\nor, in the familiar form of minimizing a penalized loss function,\n$$\n\\min_{\\beta,\\beta_0}  \\sum_i [1- y_i(x_i \\beta +\\beta_0) ]_+ + \\lambda \\lVert \\beta \\rVert^2,$$ {#eq-svm-soft-hingeloss}\nwhere $[1-y_i h(x_i)]_+:=\\max\\{1-y_i h(x_i),0\\}$ is known as the *hinge\nloss*, operating on the margin $y_i h(x_i)=y_i(x_i \\beta +\\beta_0)$. It\nmeasures the cost of $x_i$ being on the wrong side of the separating\nhyperplane.\n\nThe class below implements from scratch an SVC in `python`, using only `numpy`. It mimics the widely used interface of `sklearn`, and solves the problem in  @eq-svm-soft-hingeloss using either _stochastic gradient descent (SGD)_ or the _BFGS_ optimizer in `scipy`.\n\n::: {#9c4e76e7 .cell execution_count=2}\n``` {.python .cell-code}\nfrom dataclasses import dataclass\nfrom scipy.optimize import minimize\n\n@dataclass\nclass svm:\n    lambd : float=1/1e10\n    grad_step : float=0.001\n    nepochs : int=1000\n    \n    def loss(self,coeff,X,y):\n        n,p= X.shape\n        alpha=coeff[0];beta=coeff[1:]\n        loss=0\n        for i,xi in enumerate(X):\n            hingeloss=1-y[i]*(alpha+np.dot(xi,beta))\n            loss+=hingeloss*(hingeloss>=0)  \n        return loss/n+2*self.lambd*np.sum(beta**2)\n        \n    def fit_bfgs(self,X,y):\n        coeff=np.zeros(X.shape[1]+1)\n        res = minimize(self.loss,coeff,method='BFGS',args=(X,y),\n                       tol = 1e-6, \n                       options={'gtol': 1e-6, 'disp': False,'maxiter':300})\n        self.beta = res.x[1:]\n        self.alpha = res.x[0] \n        self.loss = [res.fun]\n        return self\n    \n    def fit_sgd(self,X,y):\n        n,p=X.shape\n        self.alpha=1\n        self.beta=np.ones(p)\n        y=y.reshape(-1,1)\n        for _ in range(self.nepochs):\n            for i,xi in enumerate(X):\n                hingeloss = 1-y[i]*(self.alpha+np.dot(xi,self.beta))\n                dalpha=-y[i]*(hingeloss>=0)\n                dbeta= -y[i]*xi*(hingeloss>=0)+2*self.lambd*self.beta\n                self.alpha-=self.grad_step * dalpha\n                self.beta-=self.grad_step * dbeta\n        return self\n    \n    def predict(self,X):\n        return np.where(self.alpha+X.dot(self.beta)>=0,1,-1)\n\n```\n:::\n\n\nFor the separated observations case in @fig-svm-perfect-separation, both the SGD and BFGS produce separating hyperplanes with intercepts close to zero and slopes close to 1, as expected, similar to  `scikit-learn`, which uses the dual problem @eq-svm-dual to be detailed next, instead of @eq-svm-soft-hingeloss\n\n::: {#tbl-svc .cell tbl-cap='Optimal-margin hyperplane using SGD, BFGS and the scikit-learn implementation of SVC. Exact values are 0 and 1.' execution_count=3}\n``` {.python .cell-code}\nimport warnings\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\nfrom sklearn.svm import SVC # \"Support vector classifier\"\n\n\nwarnings.simplefilter('ignore')\n\ndef run_models():\n    m1=svm()\n    m1.fit_sgd(X,y)\n    intercept1 = -m1.alpha/m1.beta[1]\n    slope1 = - m1.beta[0]/m1.beta[1]\n    m2=svm()\n    m2.fit_bfgs(X,y)\n    intercept2 = -m2.alpha/m2.beta[1]\n    slope2 = - m2.beta[0]/m2.beta[1]\n\n    m3 = SVC(kernel='linear', C=1E10)\n    m3.fit(X, y)\n    intercept3 = m3.intercept_\n    slope3 = - m3.coef_[0,0]/m3.coef_[0,1]\n\n    return (intercept1,slope1,intercept2,slope2,intercept3,slope3)\n\nintercept1,slope1,intercept2,slope2,intercept3,slope3 =run_models()\n\ntable = [['SGD',intercept1,slope1],['BFGS',intercept2,slope2],['sklearn',intercept3,slope3],['exact',0,1]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Method\",\"Intercept\", \"Slope\"]\n))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=3}\nMethod      Intercept    Slope\n--------  -----------  -------\nSGD        -0.030219   1.09121\nBFGS        0.0116883  1.00174\nsklearn     0.0735623  1.02817\nexact       0          1\n:::\n:::\n\n\n@fig-svm-overlap illustrates the SGD and BFGS-based SVM routine applied to overlapping observations:\n\n::: {#cell-fig-svm-overlap .cell execution_count=4}\n``` {.python .cell-code}\nX,y,M,svidx = gen_data(spread = -.2)\nfig,ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nL = run_models()\nlabels = ['SGD','BFGS']\ncolors = ['blue','green']\nxfit = np.linspace(-2**.5, 2**.5)\nfor i in range(2):\n    ax.plot(xfit,L[2*i]+L[2*i+1]*xfit,color = colors[i],label=labels[i])\nax.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Overlapping classes](2023-11-30-support-vector-machines_files/figure-html/fig-svm-overlap-output-1.png){#fig-svm-overlap width=583 height=411}\n:::\n:::\n\n\n## Nonlinear boundaries\n\nSVMs can be extended to identify highly non-linear boundaries through _kernelization_ (the \"kernel trick\"). The idea is to map the covariate space $\\mathbb{R}^p$ into a higher dimensional space $H$, and find a separating hyperplane in the bigger space. For example, with 1-dimensional covariates, if observations in class $1$ fall in the interval $(-1,1)$ while observations of class $-1$ belong to $(-3,-2) \\cup (2,3)$, they cannot be linearly separated (cutting the real line in two). However, if we map the covariate $x$ into a point $(x,x^2)$ in a 2-dimensional space, any horizontal line with height between 1 and 4 linearly separates the observations. \n\n\nWe show in the next section that given a  function $K: \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ with some suitable properties (a _kernel_), there exists a  (possibly infinitely dimensional) Hilbert space H - the _reproducing kernel Hilbert space (RKHS)_ and  a mapping $\\Psi$ from the original feature space $\\mathbb{R}^p$ to $H$ with the property that the inner products in the new space have the _reproducing property_ in that they can be directly calculated using $K$,\n$$\n\\langle \\Psi(x),\\Psi(y) \\rangle = K(x,y).\n$$ {#eq-svm-reproducing-property}\n\nWe solve now @eq-svm-soft-margin in this higher dimensional space, with $x_i$ replaced by $\\Psi(x_i)$. Heuristically, I solve\nthe problem as if it remains finite dimensional. Let $(\\alpha_i)$ be the\nLagrange multipliers for constraints, and $\\mu_i$ the multiplier for\n$\\xi_i$. The Lagrangian is \n$$\nL:=\\frac 12 \\lVert \\beta \\rVert^2 + C \\sum_{i=1}^n \\xi_i - \\sum_i \\alpha_i (y_i(\\Psi(x_i) \\beta +\\beta_0) - 1+\\xi_i)  -\\sum_i \\mu_i \\xi_i.$$ {#eq-svm-lagrange}\n\nThe first order conditions with respect to $\\beta, \\beta_0, \\xi_i$ give\n$$\n\\beta = \\sum_{i=1}^n \\alpha_i y_i \\Psi(x_i), \\sum_i \\alpha_i y_i =0, C =\\alpha_i +\\mu_i.$$ {#eq-svm-foc}\nSubstituting @eq-svm-foc into @eq-svm-lagrange,\n$$\\begin{aligned}\nL =  \\frac 12 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i   y_j \\langle \\Psi(x_i),\\Psi(x_j) \\rangle - \\sum_i \\alpha_i (y_i \\Psi(x_i)\\sum_j \\alpha_j y_j \\Psi(x_j) -1) \\\\\n=\\sum_i \\alpha_i - \\frac 12 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i   y_j  K(x_i,x_j).\\end{aligned}$$ {#eq-svm-lagrange2}\n\n@eq-svm-lagrange2 depends only on the $n \\times n$  _Gram matrix_ of inner products  $(\\langle \\Psi(x_i),\\Psi(x_j) \\rangle)$, or equivalently, on $(K(x_i,x_j))$. We obtained the equivalent, dual form of @eq-svm-soft-margin generalized to non-linear boundaries,\n$$\\begin{aligned}\n\\min_{\\alpha_i} \\sum_i \\alpha_i - \\frac 12 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i   y_j  K(x_i,x_j) \\\\\n\\text{s.t. } \\sum_i \\alpha_i y_i =0,  0 \\le \\alpha_i \\le C \\quad \\forall i .\\end{aligned}$$ {#eq-svm-dual}\n\nDedicated optimization routines easily handle such\nquadratic problems with linear and box constraints.  @fig-svm-nonlinear  illustrates how a SVM with radial kernel\n$$\nK(x,y) = \\exp(- \\lVert x-y \\rVert^2 /(2\\sigma^2))\n$$ {#eq-svm-radial-kernel}\nis able to detect nonlinear boundaries between classes.\n\n::: {#cell-fig-svm-nonlinear .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(100, factor=.1, noise=.1)\n\nmodel = SVC(kernel='rbf').fit(X, y)\n\nfig,ax = plt.subplots()\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n\n# make grid\nx = np.linspace(xlim[0], xlim[1], 20)\ny = np.linspace(ylim[0], ylim[1], 20)\nY, X = np.meshgrid(y, x)\nxy = np.concatenate((X.reshape(-1,1), Y.reshape(-1,1)), axis = 1)\nH = model.decision_function(xy).reshape(X.shape)\n\n# plot decision boundary and margins\nax.contour(X, Y, H, levels=[0], alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Nonlinear boundary](2023-11-30-support-vector-machines_files/figure-html/fig-svm-nonlinear-output-1.png){#fig-svm-nonlinear width=582 height=411}\n:::\n:::\n\n\n## Reproducing Kernel Hilbert Space (RKHS) and the Kernel Trick\n\nLet $X \\subset \\mathbb{R}^p$ be the set of possible covariates. \nA two-dimensional function $K(x,y):X \\times X \\to \\mathbb{R}$ can be\ninterpreted as an infinite matrix, in the same way that a function\nfrom $X$ to $\\mathbb{R}$ can be viewed as a vector.\nTherefore, the matrix terminology carries over. We say that\n$K(x,y)$ is a *kernel* function if it is symmetric and positive\ndefinite, that is, if $K(x,y)=K(y,x)$ and\n$\\int \\int f(x) K(x,y) f(y)dx dy \\ge 0$ for all $f$.\n\nProving that $K$ in @eq-svm-dual is positive definite is usually done by proving the various properties of kernels. Scaling, sums, products, limits, powers and exponentiation of kernels preserve the kernel property, and the radial kernel can be obtained from the linear kernel $\\tilde{K}(x,y) = \\langle x,y \\rangle$.\n\nAnalogous to matrices, $K$ has  (a countable number of) eigenvalues $(\\lambda_n)$ and eigenvectors $(\\psi_n)$ with $\\psi_n:X \\rightarrow \\mathbb{R}$ satisfying\n$$\\int K(x,y) \\psi_n(y) dy =\\lambda_n \\psi_n(x), \\text{for all } x,n.$$ {#eq-svm-eigen}\nBy the usual arguments, the eigenvalues are real (by symmetry) and positive (by the positive definite property), and the \neigenvectors are orthogonal with respect to the inner product\n$$\n\\langle \\psi_i, \\psi_j \\rangle := \\int \\psi_i(x) \\psi_j(x) dx =0, \\; i \\neq j.\n$$ {#eq-svm-eigenvectors}\n\nMercer's theorem states that, as in the matrix case,  a kernel admits a spectral decomposition,\n\n$$\nK(x,y) = \\sum_{n=1}^\\infty \\lambda_n \\psi_n(x) \\psi_n(y),$$ {#eq-svm-spectral}\nThis ensures that $(\\phi_n)$ is an orthogonal basis for the space of functions\n$H:=\\{K(x,\\cdot) \\; : \\; x \\in X \\}$, since\n$$\nK(x,\\cdot) =  \\sum_{n=1}^\\infty \\lambda_n \\psi_n(x) \\psi_n.$$ {#eq-svm-basis-decomposition}\nAs long as each eigenvector  $\\psi_n$ is rescaled to have norm $1/\\sqrt{\\lambda_n}$, the inner product on $H$ has the *reproducing\nproperty*\n$$\n\\langle K(x,\\cdot),K(y,\\cdot) \\rangle= \\sum_{n=1}^\\infty \\lambda^2_n \\psi_n(x) \\psi_n(y) \\langle \\psi_n,\\psi_n \\rangle = \\sum_{n=1}^\\infty \\lambda_n \\psi_n(x) \\psi_n(y) =  K(x,y).$$ {#eq-svm-inner-product-reproducing}\n\n",
    "supporting": [
      "2023-11-30-support-vector-machines_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}