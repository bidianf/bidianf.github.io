[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "“A picture may be worth a thousand words, a formula is worth a thousand pictures” (Edsger Dijkstra).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machines (SVMs)\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nFlorin Bidian\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-30-support-vector-machines.html",
    "href": "posts/2023-11-30-support-vector-machines.html",
    "title": "Support Vector Machines (SVMs)",
    "section": "",
    "text": "SVMs are among the most researched and popular classifiers, and have deep connections with fundamental results in functional analysis. Unlike logistic regression, they work also when the classes are linearly separable, and through kernelization, they can identify nonlinear boundaries between classes.\nConsider observations \\((y_i,x_i)_{i=1}^n\\) with \\(y \\in \\{-1,1\\}\\) being the dependent variable and \\(x \\in \\mathbb{R}^p\\) the covariates. A hyperplane in \\(\\mathbb{R}^p\\) is the set of points \\(x \\in \\mathbb{R}^p\\) with the property that \\(h(x)=0\\), where \\(h : \\mathbb{R}^p \\to \\mathbb{R}\\) is an affine function \\(h(x):=\\beta x +\\beta_0\\) with \\(\\beta \\in \\mathbb{R}^p, \\beta_0 \\in \\mathbb{R}\\) , and \\[\\beta x=\\langle \\beta,x \\rangle := \\sum_{j=1}^p \\beta_j x_j \\tag{1}\\] is the inner (dot) product of the vectors \\(\\beta=(\\beta_1,\\ldots,\\beta_p)\\) and \\(x=(x_1,\\ldots,x_p)\\).\nA hyperplane divides the \\(\\mathbb{R}^p\\) space into two half-spaces. SVMs find the hyperplane that “best separates” the two classes \\(\\{-1,1\\}\\). Roughly, (most of the) class \\(1\\) observations fall in the half-space \\(\\{x\\: |\\: h(x)&gt;=0\\}\\), while (most of the) class \\(-1\\) observations belong to the half-space \\(\\{x \\: | \\: h(x)&lt;0\\}\\). A new observation with covariates \\(x\\) is will be classified as belonging to class determined by the sign of \\(h(x) = \\beta x +\\beta_0\\). While SVMs apply directly only to binary classes, they can be turned into a multiclass classifier using the one-vs-one pairwise classifications of observations, and using majority voting to determine the most likely class."
  },
  {
    "objectID": "posts/2023-11-30-support-vector-machines.html#linearly-separated-classes",
    "href": "posts/2023-11-30-support-vector-machines.html#linearly-separated-classes",
    "title": "Support Vector Machines (SVMs)",
    "section": "Linearly separated classes",
    "text": "Linearly separated classes\nThe two classes can be perfectly separated by the hyperplane \\(h\\) if \\(y_i h(x_i) \\ge 0\\) for all \\(1 \\le i \\le n\\).\nDenote by \\(\\lVert \\beta \\rVert := (\\sum_{j=1}^p \\beta_j^2)^{1/2}\\) the Euclidean norm of \\(\\beta\\). The distance from a point \\(x\\) to the hyperplane \\(h\\) can be calculated by selecting a point \\(x_0\\) belonging to the hyperplane and projecting \\(x-x_0\\) on the unit norm vector \\(\\beta/\\lVert \\beta \\rVert\\) orthogonal to the hyperplane, \\[\n\\frac{\\lvert  \\beta (x-x_0)\\rvert}{\\lVert \\beta \\rVert}   = \\frac{\\lvert \\beta x +\\beta_0 \\rvert}{\\lVert \\beta \\rVert} = \\frac{\\lvert h(x) \\rvert}{\\lVert \\beta \\rVert}.\n\\tag{2}\\]\nIf \\(\\beta\\) is normalized to have unit norm, \\(y_i h(x_i)\\) is the distance from the observation \\((y_i,x_i)\\) to the hyperplane, or the margin for this observation. With perfectly separable classes, we want to find the optimal-margin hyperplane, which is the hyperplane with the largest distance even from the closest points: \\[\n\\begin{aligned}\n\\max_{\\beta,\\beta_0,M} M  \\\\\n\\text{s.t. } y_i  \\frac{\\beta x_i +\\beta_0}{\\lVert \\beta \\rVert} \\ge M.\n\\end{aligned}\n\\tag{3}\\] Figure 1 illustrates the case of perfectly separable observations lying the plane (\\(p=2\\)), where hyperplanes are lines. The observations with optimal margin which is the distance to the hyperplane marked by the blue line, are circled. They are called support vectors, giving the algorithm its name. The support vectors determine entirely the optimal-margin hyperplane and any points with higher margin (further from the separating hyperplane) do not affect the optimal solution to Equation 3.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Rotation matrix, 45 degrees counterclockwise\na = 2**.5/2\nA = np.array([[a,-a],[a,a]])\nrng = np.random.default_rng(10000)\n\ndef gen_data(spread=.2,nobs=100,rng=rng):\n        X = (2*rng.random((nobs,2))-1) \n        #Obs x=(x_1,x_2) assigned to class 1 (-1) if x_2 &gt;= (&lt;) 0\n        y =2*(X[:,1]&gt;=0)-1\n        #Inject spread between classes\n        X[X[:,1]&gt;=0,1]+=spread/2; X[X[:,1]&lt;0,1]-=spread/2\n        if spread&gt;0:\n            # Support vectors \n            svidx1 = rng.choice(np.where(X[:,1]&gt;0)[0],2,replace=False)\n            X[svidx1,1] = spread/2\n            svidx_1 = rng.choice(np.where(X[:,1]&lt;0)[0],1,replace=False)\n            X[svidx_1,1] = -spread/2\n            svidx=np.concatenate((svidx1,svidx_1))\n            return (np.dot(X,A.T),y,spread/2,svidx)\n        else:\n            return (np.dot(X,A.T),y,spread/2,None)\n\nX,y,M,svidx = gen_data()\nfig,ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nxfit = yfit = np.linspace(-2**.5, 2**.5)\nax.plot(xfit, yfit,color='blue')\nax.set_xlim(-2**.5, 2**.54)\nax.fill_between(xfit, yfit - M/a, yfit + M/a, edgecolor='none',\n                  color='lightgray', alpha=0.5)\nax.scatter(X[svidx, 0],\n                    X[svidx, 1],\n                    s=150, linewidth=1, edgecolors='black',\n                    facecolors='none')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Perfect separation\n\n\n\n\n\nRelabel \\(\\beta'_0=\\beta_0/(M \\lVert \\beta \\rVert)\\), \\(\\beta'=\\beta/(M \\lVert \\beta \\rVert)\\), implying that \\(\\lVert \\beta' \\rVert = 1/M\\). Dropping the apostrophe, Equation 3 rewrites as \\[\\begin{aligned}\n\\min_{\\beta,\\beta_0} \\; \\lVert \\beta \\rVert \\\\\n\\text{s.t. } y_i  (\\beta x_i +\\beta_0)  \\ge 1. \\end{aligned} \\tag{4}\\]"
  },
  {
    "objectID": "posts/2023-11-30-support-vector-machines.html#overlapping-classes",
    "href": "posts/2023-11-30-support-vector-machines.html#overlapping-classes",
    "title": "Support Vector Machines (SVMs)",
    "section": "Overlapping classes",
    "text": "Overlapping classes\nIn general, perfect separation is impossible and we are content to find a “soft”-margin classifier solving the relaxed problem \\[\\begin{aligned}\n\\min_{\\beta,\\beta_0,\\xi_i} \\frac 12 \\lVert \\beta \\rVert^2 + C \\sum_{i=1}^n \\xi_i,  \\label{eq:rkhs:11}\\\\\n\\text{s.t. } \\xi_i \\ge 0, \\; y_i(x_i \\beta +\\beta_0) \\ge 1-\\xi_i, \\forall i \\notag \\end{aligned} \\tag{5}\\] The tuning parameter \\(C\\) controls the softness of the margin, and %(_i)$ are called slack variables. A large \\(C\\) implies less overlap.\nEquation 5 can be rewritten as an unconstrained optimization problem, \\[\n\\min_{\\beta,\\beta_0}  \\frac 12 \\lVert \\beta \\rVert^2 +C \\sum_i [1- y_i(x_i \\beta +\\beta_0) ]_+, \\tag{6}\\] or, in the familiar form of minimizing a penalized loss function, \\[\n\\min_{\\beta,\\beta_0}  \\sum_i [1- y_i(x_i \\beta +\\beta_0) ]_+ + \\lambda \\lVert \\beta \\rVert^2, \\tag{7}\\] where \\([1-y_i h(x_i)]_+:=\\max\\{1-y_i h(x_i),0\\}\\) is known as the hinge loss, operating on the margin \\(y_i h(x_i)=y_i(x_i \\beta +\\beta_0)\\). It measures the cost of \\(x_i\\) being on the wrong side of the separating hyperplane.\nThe class below implements from scratch an SVC in python, using only numpy. It mimics the widely used interface of sklearn, and solves the problem in Equation 7 using either stochastic gradient descent (SGD) or the BFGS optimizer in scipy.\n\n\nCode\nfrom dataclasses import dataclass\nfrom scipy.optimize import minimize\n\n@dataclass\nclass svm:\n    lambd : float=1/1e10\n    grad_step : float=0.001\n    nepochs : int=1000\n    \n    def loss(self,coeff,X,y):\n        n,p= X.shape\n        alpha=coeff[0];beta=coeff[1:]\n        loss=0\n        for i,xi in enumerate(X):\n            hingeloss=1-y[i]*(alpha+np.dot(xi,beta))\n            loss+=hingeloss*(hingeloss&gt;=0)  \n        return loss/n+2*self.lambd*np.sum(beta**2)\n        \n    def fit_bfgs(self,X,y):\n        coeff=np.zeros(X.shape[1]+1)\n        res = minimize(self.loss,coeff,method='BFGS',args=(X,y),\n                       tol = 1e-6, \n                       options={'gtol': 1e-6, 'disp': False,'maxiter':300})\n        self.beta = res.x[1:]\n        self.alpha = res.x[0] \n        self.loss = [res.fun]\n        return self\n    \n    def fit_sgd(self,X,y):\n        n,p=X.shape\n        self.alpha=1\n        self.beta=np.ones(p)\n        y=y.reshape(-1,1)\n        for _ in range(self.nepochs):\n            for i,xi in enumerate(X):\n                hingeloss = 1-y[i]*(self.alpha+np.dot(xi,self.beta))\n                dalpha=-y[i]*(hingeloss&gt;=0)\n                dbeta= -y[i]*xi*(hingeloss&gt;=0)+2*self.lambd*self.beta\n                self.alpha-=self.grad_step * dalpha\n                self.beta-=self.grad_step * dbeta\n        return self\n    \n    def predict(self,X):\n        return np.where(self.alpha+X.dot(self.beta)&gt;=0,1,-1)\n\n\nFor the separated observations case in Figure 1, both the SGD and BFGS produce separating hyperplanes with intercepts close to zero and slopes close to 1, as expected, similar to scikit-learn, which uses the dual problem Equation 12 to be detailed next, instead of Equation 7\n\n\nCode\nimport warnings\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\nfrom sklearn.svm import SVC # \"Support vector classifier\"\n\n\nwarnings.simplefilter('ignore')\n\ndef run_models():\n    m1=svm()\n    m1.fit_sgd(X,y)\n    intercept1 = -m1.alpha/m1.beta[1]\n    slope1 = - m1.beta[0]/m1.beta[1]\n    m2=svm()\n    m2.fit_bfgs(X,y)\n    intercept2 = -m2.alpha/m2.beta[1]\n    slope2 = - m2.beta[0]/m2.beta[1]\n\n    m3 = SVC(kernel='linear', C=1E10)\n    m3.fit(X, y)\n    intercept3 = m3.intercept_\n    slope3 = - m3.coef_[0,0]/m3.coef_[0,1]\n\n    return (intercept1,slope1,intercept2,slope2,intercept3,slope3)\n\nintercept1,slope1,intercept2,slope2,intercept3,slope3 =run_models()\n\ntable = [['SGD',intercept1,slope1],['BFGS',intercept2,slope2],['sklearn',intercept3,slope3],['exact',0,1]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Method\",\"Intercept\", \"Slope\"]\n))\n\n\n\n\nTable 1: Optimal-margin hyperplane using SGD, BFGS and the scikit-learn implementation of SVC. Exact values are 0 and 1.\n\n\n\n\n\n\nMethod\nIntercept\nSlope\n\n\n\n\nSGD\n-0.030219\n1.09121\n\n\nBFGS\n0.0116883\n1.00174\n\n\nsklearn\n0.0735623\n1.02817\n\n\nexact\n0\n1\n\n\n\n\n\n\n\n\nFigure 2 illustrates the SGD and BFGS-based SVM routine applied to overlapping observations:\n\n\nCode\nX,y,M,svidx = gen_data(spread = -.2)\nfig,ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nL = run_models()\nlabels = ['SGD','BFGS']\ncolors = ['blue','green']\nxfit = np.linspace(-2**.5, 2**.5)\nfor i in range(2):\n    ax.plot(xfit,L[2*i]+L[2*i+1]*xfit,color = colors[i],label=labels[i])\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Overlapping classes"
  },
  {
    "objectID": "posts/2023-11-30-support-vector-machines.html#nonlinear-boundaries",
    "href": "posts/2023-11-30-support-vector-machines.html#nonlinear-boundaries",
    "title": "Support Vector Machines (SVMs)",
    "section": "Nonlinear boundaries",
    "text": "Nonlinear boundaries\nSVMs can be extended to identify highly non-linear boundaries through kernelization (the “kernel trick”). The idea is to map the covariate space \\(\\mathbb{R}^p\\) into a higher dimensional space \\(H\\), and find a separating hyperplane in the bigger space. For example, with 1-dimensional covariates, if observations in class \\(1\\) fall in the interval \\((-1,1)\\) while observations of class \\(-1\\) belong to \\((-3,-2) \\cup (2,3)\\), they cannot be linearly separated (cutting the real line in two). However, if we map the covariate \\(x\\) into a point \\((x,x^2)\\) in a 2-dimensional space, any horizontal line with height between 1 and 4 linearly separates the observations.\nWe show in the next section that given a function \\(K: \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}\\) with some suitable properties (a kernel), there exists a (possibly infinitely dimensional) Hilbert space H - the reproducing kernel Hilbert space (RKHS) and a mapping \\(\\Psi\\) from the original feature space \\(\\mathbb{R}^p\\) to \\(H\\) with the property that the inner products in the new space have the reproducing property in that they can be directly calculated using \\(K\\), \\[\n\\langle \\Psi(x),\\Psi(y) \\rangle = K(x,y).\n\\tag{8}\\]\nWe solve now Equation 5 in this higher dimensional space, with \\(x_i\\) replaced by \\(\\Psi(x_i)\\). Heuristically, I solve the problem as if it remains finite dimensional. Let \\((\\alpha_i)\\) be the Lagrange multipliers for constraints, and \\(\\mu_i\\) the multiplier for \\(\\xi_i\\). The Lagrangian is \\[\nL:=\\frac 12 \\lVert \\beta \\rVert^2 + C \\sum_{i=1}^n \\xi_i - \\sum_i \\alpha_i (y_i(\\Psi(x_i) \\beta +\\beta_0) - 1+\\xi_i)  -\\sum_i \\mu_i \\xi_i. \\tag{9}\\]\nThe first order conditions with respect to \\(\\beta, \\beta_0, \\xi_i\\) give \\[\n\\beta = \\sum_{i=1}^n \\alpha_i y_i \\Psi(x_i), \\sum_i \\alpha_i y_i =0, C =\\alpha_i +\\mu_i. \\tag{10}\\] Substituting Equation 10 into Equation 9, \\[\\begin{aligned}\nL =  \\frac 12 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i   y_j \\langle \\Psi(x_i),\\Psi(x_j) \\rangle - \\sum_i \\alpha_i (y_i \\Psi(x_i)\\sum_j \\alpha_j y_j \\Psi(x_j) -1) \\\\\n=\\sum_i \\alpha_i - \\frac 12 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i   y_j  K(x_i,x_j).\\end{aligned} \\tag{11}\\]\nEquation 11 depends only on the \\(n \\times n\\) Gram matrix of inner products \\((\\langle \\Psi(x_i),\\Psi(x_j) \\rangle)\\), or equivalently, on \\((K(x_i,x_j))\\). We obtained the equivalent, dual form of Equation 5 generalized to non-linear boundaries, \\[\\begin{aligned}\n\\min_{\\alpha_i} \\sum_i \\alpha_i - \\frac 12 \\sum_i \\sum_j \\alpha_i \\alpha_j y_i   y_j  K(x_i,x_j) \\\\\n\\text{s.t. } \\sum_i \\alpha_i y_i =0,  0 \\le \\alpha_i \\le C \\quad \\forall i .\\end{aligned} \\tag{12}\\]\nDedicated optimization routines easily handle such quadratic problems with linear and box constraints. Figure 3 illustrates how a SVM with radial kernel \\[\nK(x,y) = \\exp(- \\lVert x-y \\rVert^2 /(2\\sigma^2))\n\\tag{13}\\] is able to detect nonlinear boundaries between classes.\n\n\nCode\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(100, factor=.1, noise=.1)\n\nmodel = SVC(kernel='rbf').fit(X, y)\n\nfig,ax = plt.subplots()\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n\n# make grid\nx = np.linspace(xlim[0], xlim[1], 20)\ny = np.linspace(ylim[0], ylim[1], 20)\nY, X = np.meshgrid(y, x)\nxy = np.concatenate((X.reshape(-1,1), Y.reshape(-1,1)), axis = 1)\nH = model.decision_function(xy).reshape(X.shape)\n\n# plot decision boundary and margins\nax.contour(X, Y, H, levels=[0], alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Nonlinear boundary"
  },
  {
    "objectID": "posts/2023-11-30-support-vector-machines.html#reproducing-kernel-hilbert-space-rkhs-and-the-kernel-trick",
    "href": "posts/2023-11-30-support-vector-machines.html#reproducing-kernel-hilbert-space-rkhs-and-the-kernel-trick",
    "title": "Support Vector Machines (SVMs)",
    "section": "Reproducing Kernel Hilbert Space (RKHS) and the Kernel Trick",
    "text": "Reproducing Kernel Hilbert Space (RKHS) and the Kernel Trick\nLet \\(X \\subset \\mathbb{R}^p\\) be the set of possible covariates. A two-dimensional function \\(K(x,y):X \\times X \\to \\mathbb{R}\\) can be interpreted as an infinite matrix, in the same way that a function from \\(X\\) to \\(\\mathbb{R}\\) can be viewed as a vector. Therefore, the matrix terminology carries over. We say that \\(K(x,y)\\) is a kernel function if it is symmetric and positive definite, that is, if \\(K(x,y)=K(y,x)\\) and \\(\\int \\int f(x) K(x,y) f(y)dx dy \\ge 0\\) for all \\(f\\).\nProving that \\(K\\) in Equation 12 is positive definite is usually done by proving the various properties of kernels. Scaling, sums, products, limits, powers and exponentiation of kernels preserve the kernel property, and the radial kernel can be obtained from the linear kernel \\(\\tilde{K}(x,y) = \\langle x,y \\rangle\\).\nAnalogous to matrices, \\(K\\) has (a countable number of) eigenvalues \\((\\lambda_n)\\) and eigenvectors \\((\\psi_n)\\) with \\(\\psi_n:X \\rightarrow \\mathbb{R}\\) satisfying \\[\\int K(x,y) \\psi_n(y) dy =\\lambda_n \\psi_n(x), \\text{for all } x,n. \\tag{14}\\] By the usual arguments, the eigenvalues are real (by symmetry) and positive (by the positive definite property), and the eigenvectors are orthogonal with respect to the inner product \\[\n\\langle \\psi_i, \\psi_j \\rangle := \\int \\psi_i(x) \\psi_j(x) dx =0, \\; i \\neq j.\n\\tag{15}\\]\nMercer’s theorem states that, as in the matrix case, a kernel admits a spectral decomposition,\n\\[\nK(x,y) = \\sum_{n=1}^\\infty \\lambda_n \\psi_n(x) \\psi_n(y), \\tag{16}\\] This ensures that \\((\\phi_n)\\) is an orthogonal basis for the space of functions \\(H:=\\{K(x,\\cdot) \\; : \\; x \\in X \\}\\), since \\[\nK(x,\\cdot) =  \\sum_{n=1}^\\infty \\lambda_n \\psi_n(x) \\psi_n. \\tag{17}\\] As long as each eigenvector \\(\\psi_n\\) is rescaled to have norm \\(1/\\sqrt{\\lambda_n}\\), the inner product on \\(H\\) has the reproducing property \\[\n\\langle K(x,\\cdot),K(y,\\cdot) \\rangle= \\sum_{n=1}^\\infty \\lambda^2_n \\psi_n(x) \\psi_n(y) \\langle \\psi_n,\\psi_n \\rangle = \\sum_{n=1}^\\infty \\lambda_n \\psi_n(x) \\psi_n(y) =  K(x,y). \\tag{18}\\]"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "You can access my academic research papers here.\n\nI refereed for Econometrica, Journal of Economic Theory, Journal of Mathematical Economics, Economic Theory, Journal of Empirical Finance, Review of Financial Economics, Optimization.\n\nI presented my research at:\nSAET Faro, PT, EWGET Salamanca, ES, Georgia State University (2017), University of Tokyo, Bank of Japan Tokyo, JP, XXV EWGET Glasgow, GB, INFER Reus, ES, ERMAS Timisoara, RO, GEBA Iasi, RO (2016), ERMAS Cluj, RO (2015), EEA-ESEM Toulouse, FR, ERMAS Cluj, RO, INFER Timisoara, RO (2014), Brown University, Yeshiva University, AMES SG, China Meetings of the Econometric Society Beijing, CN, XXII EWGET Vienna, AT, MBF Rome, IT (2013), LAMES Lima, PE, NASM Evanston, US, XXI EWGET Exeter, UK (2012), Utah State University, SAET Faro, PT, Georgia State University (2011)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Florin Bidian",
    "section": "",
    "text": "I am a Professor of the Practice in Economics and Data Science at Northeastern University, Seattle. A recent resume is available here.\n\nEducation\nPhD in Economics, 2011, University of Minnesota\n\n\nInterests\nEconomics, Statistics, Machine Learning, Applied Mathematics"
  }
]